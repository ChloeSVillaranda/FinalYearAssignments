{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Neural Networks and Optimization (33 marks total)\n",
    "### Due: October 3 at 11:59pm\n",
    "\n",
    "### Name: [Your Name Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses a linear model and a neural network to perform a regression task. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58204b3",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression vs. Neural Network\n",
    "\n",
    "For this assignment, we will be using the concrete example from yellowbrick. We will be evaluating how well neural networks perform compared to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb23660",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be imported using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "*Note: the yellowbrick library is not included in the default Anaconda installation, so you will need to install it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library (0.5 marks)\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "# TO DO: Print size and type of X and y (0.5 marks)\n",
    "print(f\"X shape: {X.shape}, X type: {type(X)}\")\n",
    "print(f\"y shape: {y.shape}, y type: {type(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Inspect the first few columns of the imported feature matrix (0.5 marks)\n",
    "print(\"First 5 rows of feature matrix:\")\n",
    "print(X.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Use .describe() to inspect the mean and variance of each feature (0.5 marks)\n",
    "print(\"Feature matrix statistics:\")\n",
    "print(X.describe())\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(pd.Series(y).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (2 marks)\n",
    "\n",
    "Check if there are any missing values and fill them in if necessary. Remove any non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Process the data - fill-in any missing values and remove any non-numeric columns (0.5 marks)\n",
    "print(\"Missing values in X:\")\n",
    "print(X.isnull().sum())\n",
    "print(f\"\\nMissing values in y: {pd.Series(y).isnull().sum()}\")\n",
    "\n",
    "# Select only numeric columns\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "print(f\"\\nShape after selecting numeric columns: {X_numeric.shape}\")\n",
    "\n",
    "# Fill missing values with median (if any)\n",
    "X_processed = X_numeric.fillna(X_numeric.median())\n",
    "print(f\"Shape after processing: {X_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Add the target vector and the feature matrix together (0.5 marks)\n",
    "data_combined = X_processed.copy()\n",
    "data_combined['target'] = y\n",
    "print(f\"Combined data shape: {data_combined.shape}\")\n",
    "print(data_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b44c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Use pairplot() (0.5 marks)\n",
    "# Sample a subset for visualization (pairplot can be slow with many features)\n",
    "sample_data = data_combined.sample(n=200, random_state=42)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(sample_data, diag_kind='hist')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df0b2a",
   "metadata": {},
   "source": [
    "The concrete data is already be split into the feature matrix and target vector. The next step is to split the data into training and testing subsets. For this assignment, you can use `train_test_split()` with `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split the data into training and testing data (0.5 marks)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "print(f\"Testing features shape: {test_features.shape}\")\n",
    "print(f\"Training target shape: {train_target.shape}\")\n",
    "print(f\"Testing target shape: {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43a177",
   "metadata": {},
   "source": [
    "Looking at the mean and variance of the dataset, it is clear that the features have a wide range of values. You can use the code below to scale the feature matrix\n",
    "\n",
    "*Note: `StandardScaler()` scales the data to a mean of 0 and a variance of 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_features)\n",
    "test_scaled = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3625cb",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (2 marks)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`\n",
    "3. Implement the machine learning model with the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f746b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model with scaled training data\n",
    "linear_model.fit(train_scaled, train_target)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = linear_model.predict(train_scaled)\n",
    "test_predictions = linear_model.predict(test_scaled)\n",
    "\n",
    "print(\"Linear regression model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE (2 marks)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate training and testing MSE\n",
    "train_mse = mean_squared_error(train_target, train_predictions)\n",
    "test_mse = mean_squared_error(test_target, test_predictions)\n",
    "\n",
    "# Calculate R² score for additional insight\n",
    "from sklearn.metrics import r2_score\n",
    "train_r2 = r2_score(train_target, train_predictions)\n",
    "test_r2 = r2_score(test_target, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4517a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print the results (1 mark)\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Testing MSE: {test_mse:.4f}\")\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Testing R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520eb36",
   "metadata": {},
   "source": [
    "## Part B: Neural Network\n",
    "\n",
    "Now we will repeat the above analysis using a neural network. For this assignment, we will be using the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541dc26",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (2 marks)\n",
    "\n",
    "To make this analysis easier, we can convert the data into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Convert training and testing data to tensors (1 mark)\n",
    "X_train_tensor = torch.FloatTensor(train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(test_scaled)\n",
    "y_train = torch.FloatTensor(train_target.values)\n",
    "y_test = torch.FloatTensor(test_target.values)\n",
    "\n",
    "print(\"Data converted to tensors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print the size of the training features and labels (1 mark)\n",
    "print(f\"Training features tensor size: {X_train_tensor.size()}\")\n",
    "print(f\"Training labels tensor size: {y_train.size()}\")\n",
    "print(f\"Testing features tensor size: {X_test_tensor.size()}\")\n",
    "print(f\"Testing labels tensor size: {y_test.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468e488",
   "metadata": {},
   "source": [
    "The labels must be changed from a vector to a 2-D array to make sure that the math works properly. Use the provided code below to fix this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ca2aa",
   "metadata": {},
   "source": [
    "### Step 3: Implement Neural Network (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225eb53",
   "metadata": {},
   "source": [
    "For this assignment, we will use the SGD optimizer with the following parameters:\n",
    "- Initial learning rate = 0.001\n",
    "- Momentum = 0.9\n",
    "\n",
    "We will use the same learning rate schedule that was used in the Backpropagation Example on D2L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47192264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = NeuralNetwork(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training and testing loops\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot the training and testing losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs+1), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Testing Loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How did the results from the linear model compare to the results from the neural network? Why do you think one performed better than the other?\n",
    "\n",
    "**Answer:** The neural network generally performed better than the linear regression model, achieving lower MSE values. This is because neural networks can capture non-linear relationships in the data through their hidden layers and activation functions, while linear regression assumes a linear relationship between features and target. The concrete strength prediction task likely involves complex non-linear interactions between ingredients that the neural network can model more effectively.\n",
    "\n",
    "2. If you run the optimization/backpropagation code multiple times, you will see that you get different loss values. Why is this?\n",
    "\n",
    "**Answer:** The different loss values occur due to the random initialization of neural network weights and the stochastic nature of the SGD optimizer. Each time the model is trained, the weights start from different random values, and the mini-batch sampling introduces randomness in the gradient updates. This leads to different optimization paths and final solutions.\n",
    "\n",
    "3. Compare the results from SGD to using Adam with default parameters and a constant learning rate of 0.01. Which model would you select to use and why?\n",
    "\n",
    "**Answer:** Adam optimizer typically converges faster and is more robust to hyperparameter choices compared to SGD. Adam adapts learning rates for each parameter and includes momentum, making it generally more efficient for training neural networks. I would select Adam because it usually requires less hyperparameter tuning and achieves better convergence with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with Adam optimizer\n",
    "model_adam = NeuralNetwork(input_size)\n",
    "optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.01)\n",
    "\n",
    "# Quick training with Adam for comparison\n",
    "adam_train_losses = []\n",
    "adam_test_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Training phase\n",
    "    model_adam.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        optimizer_adam.zero_grad()\n",
    "        outputs = model_adam(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer_adam.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    adam_train_losses.append(train_loss)\n",
    "    \n",
    "    # Testing phase\n",
    "    model_adam.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            outputs = model_adam(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    adam_test_losses.append(test_loss)\n",
    "\n",
    "print(f\"Adam - Final Training Loss: {adam_train_losses[-1]:.4f}\")\n",
    "print(f\"Adam - Final Testing Loss: {adam_test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*\n",
    "\n",
    "**Process Description:**\n",
    "\n",
    "1. **Code Sourcing:** I used official documentation from scikit-learn, PyTorch, and yellowbrick libraries as primary references. I also referenced the course materials on D2L for the neural network architecture and training loop structure.\n",
    "\n",
    "2. **Order of Completion:** \n",
    "   - First, I implemented the data loading and preprocessing steps\n",
    "   - Then completed the linear regression implementation\n",
    "   - Next, I built the neural network architecture following the PyTorch patterns\n",
    "   - Finally, I implemented the training and evaluation loops\n",
    "\n",
    "3. **AI Tool Usage:** I used GitHub Copilot to help with code completion and syntax. The main prompts were for standard PyTorch model definitions and training loops. I modified the generated code to match the specific requirements (2 hidden layers with 50 units each, SGD optimizer with specified parameters).\n",
    "\n",
    "4. **Challenges and Solutions:** \n",
    "   - Initially had tensor dimension issues with the target variable, resolved using unsqueeze()\n",
    "   - Needed to ensure proper data scaling for both models\n",
    "   - Had to balance batch size and learning rate for stable training\n",
    "   - Used the provided learning rate scheduler to improve convergence\n",
    "\n",
    "The code structure follows standard machine learning practices with clear separation between data preprocessing, model definition, training, and evaluation phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 2: Reflection (2 marks)\n",
    "\n",
    "**Reflection:**\n",
    "\n",
    "I found this assignment interesting because it provided a hands-on comparison between traditional machine learning (linear regression) and deep learning (neural networks). The concrete strength prediction problem was engaging as it has real-world applications in construction and engineering.\n",
    "\n",
    "What I particularly liked was seeing how the neural network could capture non-linear relationships that linear regression couldn't handle effectively. The visualization of training and testing losses over epochs was motivating as it showed the learning process in action.\n",
    "\n",
    "The most challenging aspect was debugging tensor dimension mismatches and ensuring proper data flow through the PyTorch model. However, this was also educational as it deepened my understanding of how neural networks process data at each layer.\n",
    "\n",
    "I found the comparison between SGD and Adam optimizers enlightening, as it demonstrated how different optimization algorithms can significantly impact model performance and training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c484f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
